{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsuM/zxwumJK6/BzGNVJHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonathanarbel/AI-LAW/blob/main/Class_3_in_Class_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF_X9SZG93BK"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "\n",
        "# Configure the API key\n",
        "API_KEY = \"AIzaSyAmGmHuMeKN-HNKor7TItl4uSlDeHLZAd\"\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "# Create an embedding model\n",
        "model = \"models/embedding-001\"\n",
        "\n",
        "\n",
        "# Create a list of strings\n",
        "strings = [\n",
        "    \"King\",\n",
        "    \"Queen\",\n",
        "    \"Man\",\n",
        "    \"Woman\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Generate embeddings for each string\n",
        "embeddings = []\n",
        "for string in strings:\n",
        "    embedding = genai.embed_content(\n",
        "        model=model,\n",
        "        content=string,\n",
        "        task_type=\"retrieval_document\"\n",
        "    )\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Print the embeddings and their dimensionality\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    print(f\"Embedding for string {i + 1}:\")\n",
        "    print(embedding)\n",
        "    print(f\"This embedding has {len(embedding['embedding'])} dimensions.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Print the statement about dimensionality\n",
        "print(f\"Each string has {len(embeddings[0]['embedding'])} dimensions. That's quite long!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'embeddings' is your list of embeddings from the previous cell\n",
        "# Extract the embedding vectors\n",
        "embedding_vectors = [emb['embedding'] for emb in embeddings]\n",
        "\n",
        "# Convert to numpy array\n",
        "embedding_array = np.array(embedding_vectors)\n",
        "\n",
        "# Check the number of samples\n",
        "n_samples = embedding_array.shape[0]\n",
        "\n",
        "if n_samples < 4:\n",
        "    # Use PCA for very small datasets\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_2d = pca.fit_transform(embedding_array)\n",
        "    method = \"PCA\"\n",
        "else:\n",
        "    # Use t-SNE with adjusted perplexity\n",
        "    perplexity = min(30, n_samples - 1)  # Adjust perplexity based on sample size\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "    embeddings_2d = tsne.fit_transform(embedding_array)\n",
        "    method = \"t-SNE\"\n",
        "\n",
        "# Plot the 2D embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
        "\n",
        "# Label each point\n",
        "for i, txt in enumerate(strings):\n",
        "    plt.annotate(txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "\n",
        "plt.title(f\"2D Representation of Embeddings using {method}\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bS4Ay-WDBEy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate pairwise distances\n",
        "distances = np.linalg.norm(embeddings_2d[:, None] - embeddings_2d, axis=2)\n",
        "\n",
        "# Plot vectors and distances\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Draw vectors\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    plt.quiver(0, 0, x, y, angles='xy', scale_units='xy', scale=1, label=strings[i])\n",
        "\n",
        "# Draw distances\n",
        "for i in range(len(embeddings_2d)):\n",
        "    for j in range(i+1, len(embeddings_2d)):\n",
        "        x1, y1 = embeddings_2d[i]\n",
        "        x2, y2 = embeddings_2d[j]\n",
        "        plt.plot([x1, x2], [y1, y2], 'r--', alpha=0.3)\n",
        "        midpoint = ((x1+x2)/2, (y1+y2)/2)\n",
        "        plt.text(midpoint[0], midpoint[1], f\"{distances[i,j]:.2f}\", fontsize=8)\n",
        "\n",
        "plt.title(\"Vectors and Distances between Word Embeddings\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.legend()\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print pairwise distances\n",
        "print(\"Pairwise Distances:\")\n",
        "for i in range(len(strings)):\n",
        "    for j in range(i+1, len(strings)):\n",
        "        print(f\"Distance between '{strings[i]}' and '{strings[j]}': {distances[i,j]:.2f}\")\n",
        "\n",
        "# Calculate and print vector operations\n",
        "print(\"\\nVector Operations:\")\n",
        "king = embedding_vectors[strings.index(\"King\")]\n",
        "queen = embedding_vectors[strings.index(\"Queen\")]\n",
        "man = embedding_vectors[strings.index(\"Man\")]\n",
        "woman = embedding_vectors[strings.index(\"Woman\")]\n",
        "\n",
        "result = np.array(queen) - np.array(king) + np.array(man)\n"
      ],
      "metadata": {
        "id": "524lH-vn-mq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNxLhZxgCsYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get embeddings for \"King\", \"Man\", and \"Woman\"\n",
        "king_embedding = embeddings[0]['embedding']\n",
        "man_embedding = embeddings[2]['embedding']\n",
        "woman_embedding = embeddings[3]['embedding']\n",
        "\n",
        "# Perform the arithmetic operation: King - Man + Woman\n",
        "result_embedding = np.array(king_embedding) - np.array(man_embedding) + np.array(woman_embedding)\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_a = np.linalg.norm(vec1)\n",
        "    norm_b = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "# Compare the result_embedding with the available embeddings and print cosine similarity\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    similarity = cosine_similarity(result_embedding, embedding['embedding'])\n",
        "\n",
        "\n",
        "# Compare the result_embedding with the available embeddings and calculate cosine similarity\n",
        "similarities = []\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    similarity = cosine_similarity(result_embedding, embedding['embedding'])\n",
        "    similarities.append((strings[i], round(similarity, 3)))\n",
        "\n",
        "# Sort the results by similarity\n",
        "sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted results\n",
        "for string, similarity in sorted_similarities:\n",
        "    print(f\"Cosine similarity between result and string {string}: {similarity}\")\n"
      ],
      "metadata": {
        "id": "gwoI4IPtB44p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}